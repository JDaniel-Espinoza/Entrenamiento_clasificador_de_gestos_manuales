{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MCHGR/Modelos/blob/main/Tercer_modelo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3G5PtAUFe0k"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import tensorflow as tf \n",
        "#import matplotlib.pyplot as plt /content/drive/MyDrive/dataset/dataset/real/images/Drag \n",
        "ruta_Drag1 = \"/content/drive/MyDrive/Dataset2/data/four_short\"\n",
        "\n",
        "Drag_train = []\n",
        "img_size = 150\n",
        "\n",
        "for img in os.listdir(ruta_Drag1):\n",
        "  img = cv2.imread(os.path.join(ruta_Drag1,img))\n",
        "#img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  img_gray_resize = cv2.resize(img,(img_size,img_size))\n",
        "  Drag_train.append([img_gray_resize])\n",
        "\n",
        "#print(Drag_train.shape)\n",
        "#print(Drag_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAjRLAm-Hdqs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(np.squeeze(train_Images[1000]))\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnEf0Ft3K5ko",
        "outputId": "bbbc90ee-8f03-431e-d903-e1097486feae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRxqvEWEsoTT"
      },
      "outputs": [],
      "source": [
        "#/content/drive/MyDrive/dataset/dataset/real/images/Loupe\n",
        "ruta_Loupe1 = \"/content/drive/MyDrive/Dataset2/data/one_short\"\n",
        "Loupe_train = []\n",
        "img_size2 = 150\n",
        "\n",
        "\n",
        "\n",
        "for img2 in os.listdir(ruta_Loupe1):\n",
        "  img2 = cv2.imread(os.path.join(ruta_Loupe1,img2))\n",
        "# img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  img_gray_resize2 = cv2.resize(img2,(img_size2,img_size2))\n",
        "  Loupe_train.append([img_gray_resize2])\n",
        "#  print(len(Drag_train))\n",
        "#  print(Drag_train)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTs6wAy1XwcR"
      },
      "outputs": [],
      "source": [
        "#/content/drive/MyDrive/dataset/dataset/real/images/Loupe\n",
        "ruta_peace = \"/content/drive/MyDrive/Dataset2/data/peace_short\"\n",
        "peace_train = []\n",
        "img_size2 = 150\n",
        "\n",
        "\n",
        "\n",
        "for img2 in os.listdir(ruta_peace):\n",
        "  img2 = cv2.imread(os.path.join(ruta_peace,img2))\n",
        "# img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  img_gray_resize2 = cv2.resize(img2,(img_size2,img_size2))\n",
        "  peace_train.append([img_gray_resize2])\n",
        "#  print(len(Drag_train))\n",
        "#  print(Drag_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM3Bf2Kuh0Pm",
        "outputId": "a47f11fd-960b-4cec-89d9-14ebd2014ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5181, 1, 150, 150, 3)\n",
            "(5139, 1, 150, 150, 3)\n",
            "(5402, 1, 150, 150, 3)\n"
          ]
        }
      ],
      "source": [
        "peace_train = np.array(peace_train)\n",
        "print(peace_train.shape)\n",
        "\n",
        "Drag_train = np.array(Drag_train)\n",
        "print(Drag_train.shape)\n",
        "\n",
        "Loupe_train = np.array(Loupe_train)\n",
        "print(Loupe_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkwEUbw3Ynzo",
        "outputId": "9776cc9a-d91a-4fee-d676-c50ffd9269bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n",
            "imagenes train =  (3000, 150, 150, 3)\n",
            "1800\n",
            "imagenes val =  (1800, 150, 150, 3)\n",
            "1200\n",
            "(1200, 150, 150, 3)\n",
            "3000\n",
            "etiquetas train =  (3000,)\n",
            "1800\n",
            "etiquetas val =  (1800,)\n",
            "1200\n",
            "(1200,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, utils, callbacks\n",
        "\n",
        "\n",
        "#Drag_train = np.array(Drag_train)\n",
        "Drag_train = Drag_train.reshape((5139, 150, 150, 3)).astype(\"float32\") / 255\n",
        "train_images_D, val_images_D, test_images_D = Drag_train[0:1000], Drag_train[1000:1600], Drag_train[1600:2000]\n",
        "etiquetas_1 = np.repeat(0,1000)\n",
        "etiquetas_2 = np.repeat(0,600)\n",
        "etiquetas_3 = np.repeat(0,400)\n",
        "'''\n",
        "#Drag_train = np.array(Drag_train)\n",
        "Drag_train = Drag_train.reshape((5139, 150, 150, 3)).astype(\"float32\") / 255\n",
        "train_images_D, val_images_D, test_images_D = Drag_train[0:1000], Drag_train[1000:1600], Drag_train[1600:2000]\n",
        "etiquetas_1 = np.repeat(0,1000)\n",
        "etiquetas_2 = np.repeat(0,600)\n",
        "#etiquetas_3 = np.repeat(0,400)\n",
        "'''\n",
        "\n",
        "#Loupe_train = np.array(Loupe_train)\n",
        "Loupe_train = Loupe_train.reshape((5402, 150, 150, 3)).astype(\"float32\") / 255\n",
        "train_images_L, val_images_L, test_images_L = Loupe_train[0:1000], Loupe_train[1000:1600], Loupe_train[1600:2000]\n",
        "etiquetas_10 = np.repeat(1,1000)\n",
        "etiquetas_20 = np.repeat(1,600)\n",
        "etiquetas_30 = np.repeat(1,400)\n",
        "\n",
        "#Loupe_train = np.array(Loupe_train)\n",
        "peace_train = peace_train.reshape((5181, 150, 150, 3)).astype(\"float32\") / 255\n",
        "train_images_P, val_images_P, test_images_P = peace_train[0:1000], peace_train[1000:1600], peace_train[1600:2000]\n",
        "etiquetas_100 = np.repeat(2,1000)\n",
        "etiquetas_200 = np.repeat(2,600)\n",
        "etiquetas_300 = np.repeat(1,400)\n",
        "\n",
        "'''\n",
        "#Loupe_train = np.array(Loupe_train)\n",
        "peace_train = peace_train.reshape((5181, 150, 150, 3)).astype(\"float32\") / 255\n",
        "train_images_P, val_images_P = peace_train[0:1000], peace_train[1000:1600]\n",
        "etiquetas_100 = np.repeat(2,1000)\n",
        "etiquetas_200 = np.repeat(2,600)\n",
        "#etiquetas_300 = np.repeat(1,400)\n",
        "'''\n",
        "\n",
        "images_train = np.concatenate([train_images_D,train_images_L, train_images_P])\n",
        "print(len(images_train))\n",
        "Images_train = np.array(images_train)\n",
        "print(\"imagenes train = \",Images_train.shape)\n",
        "\n",
        "images_val = np.concatenate([val_images_D,val_images_L, val_images_P])\n",
        "print(len(images_val))\n",
        "Images_val = np.array(images_val)\n",
        "print(\"imagenes val = \",Images_val.shape)\n",
        "\n",
        "images_test = np.concatenate([test_images_D,test_images_L, test_images_P])\n",
        "print(len(images_test))\n",
        "Images_test = np.array(images_test)\n",
        "print(Images_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "etiquetas_train = np.concatenate([etiquetas_1, etiquetas_10, etiquetas_100])\n",
        "print(len(etiquetas_train))\n",
        "Etiquetas_train = np.array(etiquetas_train)\n",
        "print(\"etiquetas train = \",Etiquetas_train.shape)\n",
        "\n",
        "etiquetas_val = np.concatenate([etiquetas_2, etiquetas_20, etiquetas_200])\n",
        "print(len(etiquetas_val))\n",
        "Etiquetas_val = np.array(etiquetas_val)\n",
        "print(\"etiquetas val = \",Etiquetas_val.shape)\n",
        "\n",
        "etiquetas_test = np.concatenate([etiquetas_3, etiquetas_30, etiquetas_300])\n",
        "print(len(etiquetas_test))\n",
        "Etiquetas_test = np.array(etiquetas_test)\n",
        "print(Etiquetas_test.shape)\n",
        "\n",
        "'''\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(Images_train)\n",
        "train_batched_dataset = train_dataset.batch(32)\n",
        "for i, element in enumerate(train_batched_dataset):\n",
        "  print(element.shape)\n",
        "  if i >=2:\n",
        "    break\n",
        "'''\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, utils, callbacks\n",
        "\n",
        "def preproces(X, n, n_max):\n",
        "  Drag_train = Drag_train.reshape((n, 150, 150, 3)).astype(\"float32\") / 255\n",
        "  train_images_D, val_images_D, test_images_D = Drag_train[0:n_max*0.4], Drag_train[n_max*0.4:n_max*0.8], Drag_train[n_max*0.8:n_max]\n",
        "  etiquetas_1 = np.repeat(0,n_max*0.4)\n",
        "  etiquetas_2 = np.repeat(0,(n_max*0.8-n_max*0.4))\n",
        "  etiquetas_3 = np.repeat(0,(n_max-n_max*0.8))\n",
        "'''"
      ],
      "metadata": {
        "id": "b1ph6xoqy5Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1ZO5IK25YQv",
        "outputId": "a18c985f-bc72-4c31-b5c2-ae0012c146de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 3.5150 - accuracy: 0.3423 - val_loss: 1.2954 - val_accuracy: 0.3333\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 235s 10s/step - loss: 1.9274 - accuracy: 0.3680 - val_loss: 1.4929 - val_accuracy: 0.3333\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 234s 10s/step - loss: 1.8938 - accuracy: 0.3737 - val_loss: 1.5035 - val_accuracy: 0.3333\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 234s 10s/step - loss: 1.8931 - accuracy: 0.4213 - val_loss: 1.5386 - val_accuracy: 0.3333\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 234s 10s/step - loss: 1.5563 - accuracy: 0.4393 - val_loss: 1.3465 - val_accuracy: 0.3333\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 234s 10s/step - loss: 1.8286 - accuracy: 0.4750 - val_loss: 1.1463 - val_accuracy: 0.3333\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 233s 10s/step - loss: 1.2309 - accuracy: 0.5537 - val_loss: 1.1304 - val_accuracy: 0.3333\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 233s 10s/step - loss: 1.0804 - accuracy: 0.6000 - val_loss: 1.3549 - val_accuracy: 0.3333\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 233s 10s/step - loss: 1.1946 - accuracy: 0.6333 - val_loss: 1.5253 - val_accuracy: 0.3333\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 235s 10s/step - loss: 0.8549 - accuracy: 0.6963 - val_loss: 1.2268 - val_accuracy: 0.3333\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 235s 10s/step - loss: 0.7823 - accuracy: 0.7260 - val_loss: 1.5110 - val_accuracy: 0.3333\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 234s 10s/step - loss: 0.7054 - accuracy: 0.7717 - val_loss: 1.1466 - val_accuracy: 0.3333\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 236s 10s/step - loss: 0.6465 - accuracy: 0.7690 - val_loss: 1.1324 - val_accuracy: 0.3333\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 233s 10s/step - loss: 0.4989 - accuracy: 0.8337 - val_loss: 1.6976 - val_accuracy: 0.3333\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 233s 10s/step - loss: 0.4383 - accuracy: 0.8473 - val_loss: 1.8752 - val_accuracy: 0.3333\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.4172 - accuracy: 0.8583 - val_loss: 1.1914 - val_accuracy: 0.3333\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 238s 10s/step - loss: 0.3577 - accuracy: 0.8943 - val_loss: 1.5154 - val_accuracy: 0.3333\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.3304 - accuracy: 0.9127 - val_loss: 1.0814 - val_accuracy: 0.3422\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 236s 10s/step - loss: 0.2768 - accuracy: 0.9237 - val_loss: 1.1930 - val_accuracy: 0.3339\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 236s 10s/step - loss: 0.2198 - accuracy: 0.9413 - val_loss: 1.0744 - val_accuracy: 0.4244\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 236s 10s/step - loss: 0.2211 - accuracy: 0.9393 - val_loss: 1.3137 - val_accuracy: 0.3656\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 239s 10s/step - loss: 0.1638 - accuracy: 0.9523 - val_loss: 0.9676 - val_accuracy: 0.4894\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 237s 10s/step - loss: 0.0605 - accuracy: 0.9797 - val_loss: 0.9131 - val_accuracy: 0.5256\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 237s 10s/step - loss: 0.2738 - accuracy: 0.9417 - val_loss: 2.4577 - val_accuracy: 0.3772\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 238s 10s/step - loss: 0.0769 - accuracy: 0.9777 - val_loss: 1.6838 - val_accuracy: 0.5106\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 239s 10s/step - loss: 0.1274 - accuracy: 0.9630 - val_loss: 1.3881 - val_accuracy: 0.5939\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 239s 10s/step - loss: 0.1008 - accuracy: 0.9687 - val_loss: 0.7973 - val_accuracy: 0.7294\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 235s 10s/step - loss: 0.2464 - accuracy: 0.9550 - val_loss: 0.8747 - val_accuracy: 0.7222\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0372 - accuracy: 0.9863 - val_loss: 1.1276 - val_accuracy: 0.7378\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 237s 10s/step - loss: 0.0855 - accuracy: 0.9747 - val_loss: 1.1300 - val_accuracy: 0.7261\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 237s 10s/step - loss: 0.0201 - accuracy: 0.9950 - val_loss: 1.0064 - val_accuracy: 0.7361\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 238s 10s/step - loss: 0.0777 - accuracy: 0.9750 - val_loss: 1.1447 - val_accuracy: 0.7278\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0878 - accuracy: 0.9720 - val_loss: 1.2987 - val_accuracy: 0.6550\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 237s 10s/step - loss: 0.0236 - accuracy: 0.9930 - val_loss: 0.9013 - val_accuracy: 0.7156\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0517 - accuracy: 0.9827 - val_loss: 2.6527 - val_accuracy: 0.6450\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0534 - accuracy: 0.9837 - val_loss: 1.4248 - val_accuracy: 0.7767\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0318 - accuracy: 0.9917 - val_loss: 2.0441 - val_accuracy: 0.5889\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0528 - accuracy: 0.9860 - val_loss: 2.5181 - val_accuracy: 0.6439\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0522 - accuracy: 0.9857 - val_loss: 1.8914 - val_accuracy: 0.6244\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0726 - accuracy: 0.9850 - val_loss: 1.5035 - val_accuracy: 0.7511\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0427 - accuracy: 0.9890 - val_loss: 1.6688 - val_accuracy: 0.7517\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0423 - accuracy: 0.9857 - val_loss: 1.6944 - val_accuracy: 0.6456\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0557 - accuracy: 0.9840 - val_loss: 7.4965 - val_accuracy: 0.5667\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0347 - accuracy: 0.9927 - val_loss: 1.9512 - val_accuracy: 0.7617\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0194 - accuracy: 0.9920 - val_loss: 1.2988 - val_accuracy: 0.7772\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0580 - accuracy: 0.9790 - val_loss: 2.1455 - val_accuracy: 0.7167\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0108 - accuracy: 0.9960 - val_loss: 2.3248 - val_accuracy: 0.7111\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0137 - accuracy: 0.9953 - val_loss: 1.7412 - val_accuracy: 0.6972\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0290 - accuracy: 0.9913 - val_loss: 1.6102 - val_accuracy: 0.7022\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0758 - accuracy: 0.9823 - val_loss: 2.1014 - val_accuracy: 0.6878\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0092 - accuracy: 0.9967 - val_loss: 1.4172 - val_accuracy: 0.7800\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0358 - accuracy: 0.9900 - val_loss: 1.8444 - val_accuracy: 0.7672\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 244s 10s/step - loss: 0.0317 - accuracy: 0.9907 - val_loss: 2.1167 - val_accuracy: 0.7317\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0077 - accuracy: 0.9970 - val_loss: 1.5797 - val_accuracy: 0.6817\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0296 - accuracy: 0.9907 - val_loss: 1.3779 - val_accuracy: 0.7478\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0295 - accuracy: 0.9893 - val_loss: 2.7071 - val_accuracy: 0.7261\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 1.0984 - val_accuracy: 0.7306\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0240 - accuracy: 0.9937 - val_loss: 1.7077 - val_accuracy: 0.7494\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0285 - accuracy: 0.9913 - val_loss: 2.6225 - val_accuracy: 0.6239\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 2.5681 - val_accuracy: 0.7506\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0352 - accuracy: 0.9900 - val_loss: 1.5431 - val_accuracy: 0.7878\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0134 - accuracy: 0.9967 - val_loss: 3.8924 - val_accuracy: 0.6939\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0140 - accuracy: 0.9963 - val_loss: 1.5516 - val_accuracy: 0.8072\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0282 - accuracy: 0.9907 - val_loss: 1.1031 - val_accuracy: 0.8000\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0029 - accuracy: 0.9987 - val_loss: 1.1344 - val_accuracy: 0.7783\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0288 - accuracy: 0.9933 - val_loss: 1.9142 - val_accuracy: 0.7450\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0159 - accuracy: 0.9947 - val_loss: 1.6200 - val_accuracy: 0.7461\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0246 - accuracy: 0.9933 - val_loss: 1.4292 - val_accuracy: 0.7789\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0154 - accuracy: 0.9963 - val_loss: 1.9535 - val_accuracy: 0.7400\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0303 - accuracy: 0.9913 - val_loss: 2.0905 - val_accuracy: 0.7128\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0108 - accuracy: 0.9970 - val_loss: 1.2014 - val_accuracy: 0.7983\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0251 - accuracy: 0.9927 - val_loss: 1.4872 - val_accuracy: 0.7556\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0184 - accuracy: 0.9937 - val_loss: 2.1661 - val_accuracy: 0.7278\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0147 - accuracy: 0.9960 - val_loss: 1.4425 - val_accuracy: 0.7906\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 238s 10s/step - loss: 0.0331 - accuracy: 0.9917 - val_loss: 1.1902 - val_accuracy: 0.7894\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0176 - accuracy: 0.9960 - val_loss: 1.5370 - val_accuracy: 0.7550\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0159 - accuracy: 0.9967 - val_loss: 1.6025 - val_accuracy: 0.7789\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0028 - accuracy: 0.9987 - val_loss: 1.9332 - val_accuracy: 0.7494\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0131 - accuracy: 0.9963 - val_loss: 1.4779 - val_accuracy: 0.7961\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0273 - accuracy: 0.9910 - val_loss: 2.7800 - val_accuracy: 0.7306\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 239s 10s/step - loss: 0.0178 - accuracy: 0.9937 - val_loss: 2.3717 - val_accuracy: 0.7622\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 242s 10s/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 1.2862 - val_accuracy: 0.7894\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 241s 10s/step - loss: 0.0372 - accuracy: 0.9897 - val_loss: 1.4331 - val_accuracy: 0.7806\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 244s 10s/step - loss: 0.0075 - accuracy: 0.9973 - val_loss: 1.6093 - val_accuracy: 0.7794\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 240s 10s/step - loss: 0.0180 - accuracy: 0.9953 - val_loss: 1.4304 - val_accuracy: 0.7533\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 243s 10s/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 1.9869 - val_accuracy: 0.6994\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 238s 10s/step - loss: 0.0343 - accuracy: 0.9930 - val_loss: 1.4229 - val_accuracy: 0.8017\n",
            "Epoch 88/100\n",
            " 1/24 [>.............................] - ETA: 3:43 - loss: 0.0186 - accuracy: 0.9922"
          ]
        }
      ],
      "source": [
        "\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filts=512, kerernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "'''\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "'''\n",
        "'''\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\", padding = 'same')(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding = 'same')(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding = 'same')(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\", padding = 'same')(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\", padding = 'same')(x)\n",
        "x = layers.MaxPooling2D(pool_size=1)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=1)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=1)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)'''\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(Images_train,\n",
        "                    Etiquetas_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(Images_val,Etiquetas_val))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41kq9ICPgEJG"
      },
      "outputs": [],
      "source": [
        "test_metrics = model.evaluate(Images_test, Etiquetas_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrbSH6maUeSV",
        "outputId": "f1678189-df29-4c72-a044-518125d89ee1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/dataset/dataset/real/images/Drag')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#test_metrics = model.evaluate(Images_test, Etiquetas_test)\n",
        "import pathlib\n",
        "data_dir = pathlib.Path(\"/content/drive/MyDrive/dataset/dataset/real/images/Drag\")\n",
        "data_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBFhTFG7O-mf"
      },
      "outputs": [],
      "source": [
        "import os, shutil, pathlib\n",
        "from pathlib import Path\n",
        "original_dir = pathlib.Path(\"/content/drive/MyDrive/dataset/dataset/real/images/Drag\")\n",
        "new_base_dir = pathlib.Path(\"/content/drive/MyDrive/dataset/dataset/real/images/Dragp\")\n",
        "\n",
        "\n",
        "\n",
        "steps_per_epoch=1,\n",
        "\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    \n",
        "    dir = new_base_dir / subset_name \n",
        "    os.makedirs(dir)\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "      shutil.copyfile(src=original_dir , dst = dir )       \n",
        "\n",
        "'''def make_subset(subset_name, start_index, end_index):\n",
        "  for category in (\"cat\",\"dog\"):\n",
        "    dir = new_base_dir / subset_name / category\n",
        "    os.makedirs(dir)\n",
        "    fnames = [f\"{category}.{i}.jpg\"\n",
        "             for i in range(start_index, end_index)]\n",
        "    for fname in fnames:\n",
        "      shutil.copyfile(src=original_dir / fname,\n",
        "                      dst = dir / fname)         \n",
        "'''    \n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "make_subset(\"test\", start_index=1500, end_index=2000)\n",
        "\n",
        "#make_subset(\"train\", start_index=0, end_index=10800)\n",
        "#make_subset(\"validation\", start_index=10800, end_index=16200)\n",
        "#make_subset(\"test\", start_index=16200, end_index=27000)\n",
        "\n",
        "#data_aumentation = keras.Sequential(\n",
        "#   [\n",
        "#        layers.RandomFlip(\"horizontal\"),\n",
        "#        layers.RandomRotation(0.1),\n",
        "#        layers.RandomZoom(0.2),\n",
        "#     \n",
        "#   ]\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RNWqdcc8AMM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxvuFbaRC0TV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "Drag_train = np.array(Drag_train)\n",
        "Drag_train = Drag_train.reshape((2000, 150, 150, 3)).astype(\"float32\") / 255\n",
        "train_images, val_images, test_images = Drag_train[0:800], Drag_train[800:1600], Drag_train[1600:2000]\n",
        "etiquetas_1 = np.repeat(0,800)\n",
        "etiquetas_2 = np.repeat(0,800)\n",
        "etiquetas_3 = np.repeat(0,400)\n",
        "#train_labels, val_labels = labels[10000:], labels[:10000]\n",
        "  \n",
        "#train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "#train_batched_dataset = train_dataset.batch(32)\n",
        "#for i, element in enumerate(train_batched_dataset):\n",
        "#  print(element.shape)\n",
        "#  if i >=2:\n",
        "#    break\n",
        "#print(train_images.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owRqAzostKMt",
        "outputId": "08031f9b-6f59-4e34-906f-b9d5af5297c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2000, 150, 150, 3)\n"
          ]
        }
      ],
      "source": [
        "Loupe_train = np.array(Loupe_train)\n",
        "Loupe_train.shape = (2000, 150, 150, 3)\n",
        "print(Loupe_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgb72ooIV77s",
        "outputId": "87553c8f-1a0d-4598-9223-40a15ab6f7a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2000, 150, 150, 3)\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "Drag_train = np.array(Drag_train)\n",
        "Drag_train.shape = (2000, 150, 150, 3)\n",
        "n = Drag_train.shape[0]\n",
        "print(Drag_train.shape)\n",
        "print(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctBlexoaZeBn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as ts\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(load_img(img[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOh58XeUh4Ok"
      },
      "outputs": [],
      "source": [
        "train = Drag_train[:1000]\n",
        "val = Drag_train[1000:1600]\n",
        "test = Drag_train[1600:2000]\n",
        "print(train.shape, val.shape, test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGD7uGEhpNCO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as ts\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#y_train = to_categorical(train_labels)\n",
        "#y_test = to_categorical(test_labels)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(2, activation=\"softmax\")  \n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "x_val = val\n",
        "partial_x_train = train\n",
        "y_val = y_train[:1000]\n",
        "partial_y_train = y_train[1000:]\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(x_val, y_val))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "3tT83_8qPSKK",
        "outputId": "7e8ff105-e598-4fa0-d874-460f64cefc2f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(np.squeeze(Drag_train[-2]))\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVIuLtwGsOMP"
      },
      "outputs": [],
      "source": [
        "images = np.concatenate([Loupe_train,Drag_train])\n",
        "print(len(images))\n",
        "Images = np.array(images)\n",
        "print(Images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf6rLcmKu_wS"
      },
      "outputs": [],
      "source": [
        "etiquetas_Loupe = np.repeat(0,2000)\n",
        "print(len(etiquetas_Loupe))\n",
        "print(etiquetas_Loupe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ms0nEgQv1YR"
      },
      "outputs": [],
      "source": [
        "etiquetas_Drag = np.repeat(1,2000)\n",
        "print(len(etiquetas_Drag))\n",
        "print(etiquetas_Drag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAJXJb3mwGUY"
      },
      "outputs": [],
      "source": [
        "class_names = [\"Loupe\",\"Drag\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC4qavN_wcj5"
      },
      "outputs": [],
      "source": [
        "labels = np.concatenate([etiquetas_Loupe, etiquetas_Drag])\n",
        "print(labels.shape)\n",
        "print(labels)\n",
        "Labels = np.array(labels)\n",
        "print(Labels[3999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd3KIQBox_bH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow.keras.optimizers as Optimizer\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKNmm-sh7MWX"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(150, 150,3)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    \n",
        "    keras.layers.Dense(2, activation='softmax'),\n",
        "    \n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(Images, Labels, epochs=30)\n",
        "trained=model.fit(Images, Labels, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfyj-kWEesdD"
      },
      "outputs": [],
      "source": [
        "depronto sirve\n",
        "\n",
        "#data_dir = pathlib.Path(\"/content/drive/MyDrive/dataset/dataset/real/images/Loupe\")\n",
        "\n",
        "for img2 in os.listdir(ruta_Drag1):\n",
        "  img2 = cv2.imread(os.path.join(ruta_Drag1,img2))\n",
        "data_dir = pathlib.Path(\"img2\")\n",
        "print(data_dir)\n",
        "!ls -l img2\n",
        "params = {\n",
        "    \"directory\": data_dir,\n",
        "    \"validation_split\": 0.2,\n",
        "    \"seed\":12345,\n",
        "    \"image_size\":(180, 180),\n",
        "    \"batch_size\": 32\n",
        "}\n",
        "\n",
        "train_params = {**params,**{\"subset\":\"training\"}}\n",
        "val_params = {**params,**{\"subset\":\"validation\"}}                \n",
        "\n",
        "len(list(data_dir.glob(\"*/*.jpg\")))\n",
        "rain_ds = tf.keras.utils.image_dataset_from_directory(**train_params)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
